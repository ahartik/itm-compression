\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{url}
\usepackage{subfigure}
\usepackage{fullpage}

\newcommand{\code}{\texttt}

\title{Project in Information-Theoretic Modeling Final Report}
%\author{Aleksi Hartikainen \and Mikko Sysikaski}

\begin{document}
\maketitle

%\abstract{}

%\newpage
%\tableofcontents
%\newpage

\section{Problem 1}

The first task was to compress a sequence of bits using side data that was also available during decompression.

The file to be compressed had $100\,000$ binary digits, so trivially packing the bits yields a file of size $12\,500$ bytes.
For comparison, we also tried packing the data this way as the decompression code becomes much smaller and simpler.
This gave a total size of around 13.5~KiB uncompressed, and 12.5~KiB after compressing with gzip.

The task was to use arithmetic coding with probabilities from the given model.
Instead of using probabilities from model definition, we computed actual conditional frequencies from the data.
That is, for every tuple $(x_1,x_2,x_3,x_4)$ we counted how many times $x_0$ is 0 and 1, and this gave us the conditional probability of $x_0$ with respect to the other variables.
The calculated probabilities are shown in Figure~\ref{ex1_probs}.
We created a custom implementation of arithmetic coding that used the conditional probabilities of Figure~\ref{ex1_probs} to encode each bit.

Using arithmetic coding and the conditional probabilities, we got the data compressed in 8100~bytes.
Together with the decompression program, this gave a total file size of 9832~bytes.

To evaluate the correctness of our arithmetic coding implementation, we calculated also the theoretical bound for data size using entropy coding.
According to probabilities in Figure~\ref{ex1_probs}, the total entropy of the input file is 64790.4~bits, which is 8098.8~bytes, so the result generated by our implementation is within 1 byte of the theoretical optimum.


\begin{figure}

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $x_3$ & $x_4$ & $P(x_0 = 0)$ & $P(x_0 = 1) $ \\ \hline
   0 & 0 & 0 & 0 & 0.74244 & 0.25756 \\ \hline
   1 & 0 & 0 & 0 & 0.180631 & 0.819369 \\ \hline
   0 & 1 & 0 & 0 & 0.122035 & 0.877965 \\ \hline
   1 & 1 & 0 & 0 & 0.595336 & 0.404664 \\ \hline
   0 & 0 & 1 & 0 & 0.507643 & 0.492357 \\ \hline
   1 & 0 & 1 & 0 & 0.072675 & 0.927325 \\ \hline
   0 & 1 & 1 & 0 & 0.0461351 & 0.953865 \\ \hline
   1 & 1 & 1 & 0 & 0.359641 & 0.640359 \\ \hline
   0 & 0 & 0 & 1 & 0.888336 & 0.111664 \\ \hline
   1 & 0 & 0 & 1 & 0.37004 & 0.62996 \\ \hline
   0 & 1 & 0 & 1 & 0.277587 & 0.722413 \\ \hline
   1 & 1 & 0 & 1 & 0.78809 & 0.21191 \\ \hline
   0 & 0 & 1 & 1 & 0.733648 & 0.266352 \\ \hline
   1 & 0 & 1 & 1 & 0.177577 & 0.822423 \\ \hline
   0 & 1 & 1 & 1 & 0.119161 & 0.880839 \\ \hline
   1 & 1 & 1 & 1 & 0.582299 & 0.417701 \\ \hline
\end{tabular}
\end{center}
\caption{Conditional probabilities used in exercise 1}
\label{ex1_probs}
\end{figure}

\section{Problem 2}

The second task was to compress data consisting of stock prices of four different stocks over 10121 days.
The data consisted of floating point values with two decimals.
As we didn't want to think about floating point accuracy issues, we simply converted the initial data to integers by multiplying the values by 100 and divided our final result again by 100 before outputting it.

All the prices were under 500 so they can be stored as 16-bit integers.
Thus a trivial storing the data without any proper compression gives a total size of about 81~KiB.

To find a good compression scheme, we initially tried plotting the data, as shown in Figure~\ref{fig:ex2o}.
As there was no immedient regularity to be seen, we tried seeing how the values change by taking the differences of adjacent values, shown in Figure~\ref{fig:ex2d}.
It can be seen that there are some large negative peaks, but otherwise the differences stay close to zero.
The same behavior can be observed for each of the stocks.

\begin{figure}
    \subfigure[Original data]{\includegraphics[scale=0.5]{ex2_orig.pdf}\label{fig:ex2o}}
    \subfigure[Differences of consecutive values]{\includegraphics[scale=0.5]{ex2_diff.pdf}\label{fig:ex2d}}
    \caption{Plot of the first column of the data and the differences between its consecutive values. The original data shows a lot of irregularities, but except for a few peaks, the changes between consecutive values are quite small.}
\end{figure}

The connection between the four stocks changes didn't seem very simple so we decided to try simply compressing each of the stocks separately.
As our tests suggested that the differences between adjacent values are close to zero, we used a simple Gaussian model with mean equal to the previous value to calculate the probabilities for each value that where used in the arithmetic coding.

It seemed that the variance of the values varied quite a lot over time, so we used an adaptive scheme to decide the variance used by the normal distribution to compress each value.
The scheme works by maintaining a current approximation of variance $v_n$ and calculating new approximation $v_{n+1}$ each time we process a new value $x_{n+1}$.
The approximation is updated by taking weighted average using the formula $v_{n+1} = \frac{Lv_n + (x_{n+1}-x_n)^2}{L+1}$, where $L$ is a constant that decides how quickly the value changes.
We experimented with different values and saw that picking $L=10$ gives generally good results.

The Gaussian model with adaptive variance had a problem that the probability of the peaks is extremely low, around $10^{-100}$.
As our arithmetic coding uses 32-bit integers, it is impossible to cope with probabilities smaller than $2^{-30}$.
To fix this, we added a constant $2^{-30}$ to probabilities of all values and scaled the normal distribution probabilities so that the total probability still sums to 1.
As the largest number in the data (after converting to integers) was 45100, we only needed to add the constant to first 45101 integers, and scale the original probabilities by $\frac{2^{30}-45101}{2^{30}} \approx 0.999953$.
This factor is so close to 1 that it doesn't hinder the overall compression quality much, while adding the constant guarantees that any value can be encoded using no more than 30 bits.
We later noticed that increasing the constant further doesn't hinder the overall compression efficiency much, so we increased it to about $2^{-20}$ and decreased the scaling factor accordingly, which saved some dozens of bytes in encoding the peak values.

Another improvement with regard to the peaks was to not update the variance when the value changed too much.
Updating the variance on peaks made encoding the values following it take significantly more space than necessary.
Updating only at small changes saved several hundreds of bytes.

To compress all four stocks, we simply concatenated their numbers into one array to simplify the implementation.
Concatenating causes the values to jump when the stock to be encoded changes, but as the data had several jumps anyway, this didn't cause any additional problems.
Doing the concatenation and compressing the array using the model described above gave us total data size 44975 bytes.

\subsection{Compressing fractions and whole integers separately}

The fractional parts of the stock prices seemed to be divided quite unevenly: almost half of the values were multiples of $\frac{1}{4}$, and quite many were multiples of $\frac{1}{8}$.
Figure~\ref{fig:frac} displays the frequencies of different decimal parts.
As the differences were very big it made sense to try to take them into account in the distribution used in arithmetic coding.

The probabilities used during arithmetic coding are defined by a cumulative distribution function, and it seemed tricky to use this observation there without doing a lot of extra computation.
To avoid computation cost, we tried to simply encode the fractional parts and the whole parts separately.
The whole numbers where encoded using the Gaussian distribution described above, using 13086~bytes.
The fractional parts where encoded with a simple distribution that assumed that the probability of each symbol is proportional to the number of times it has previously been encountered, plus a small constant to avoid 0-probabilities.
This costed 24555~bytes, for a total size of 37641~bytes.

Since for all four stock there were much more multiplied of $\frac{1}{8}$ in the beginning of the data than towards the end, it made sense to encode the numbers in row-major order to exploit the changing distribution.
Using this approach with an adaptive encoding we were able to reduce the data size for fractional parts to 21364~bytes from the original 24555.

\begin{figure}
    \includegraphics[scale=0.5]{frac.pdf}
    \caption{Frequencies of different fractional values. It can be seen that multiplies of $\frac{1}{4}$ are very frequent. Multiplies of $\frac{1}{8}$ are also common, but they can't be represented exactly using just 2 decimals and they appear in the data using both of the possible roundings.}
    \label{fig:frac}
\end{figure}

\subsection {More complicated models}
We also tried fitting more complicated models to the data.
Simple polynomial fits gave significantly worse results than the first model.
Trying to fit polynomial to the difference data proved difficult as

One possible improvement to the first model we tried was to take average difference
from couple of last stock values. We used this value to predict the next change.
This approach produced few bytes longer codelength than the first model so we
abandoned the idea.
Suprisingly, however, adding a small constant value to the mean location of the normal distribution improved the compressed size by about 1~kilobyte.

Next model we considered was discrete cosine transform.
With low number of coefficients results were clearly worse than from the first model.
With 1000 coefficients theoretical codelength was approximately the same as the first model.

\begin{figure}
\begin{center}
\begin{tabular}{|c|c|}
\hline
    Degree of polynomial & Code length in bits\\ \hline
    0      & 126663 \\  \hline
    1      & 123691 \\ \hline
    2      & 123563 \\ \hline
    3      & 120854 \\ \hline
    4      & 120852 \\ \hline
    5      & 120100 \\ \hline
    6      & 120131 \\ \hline
    30     & 114236 \\ \hline
\end{tabular}
\end{center}
\caption{Code length of first stock using polynomial model}
\label{ex2_polys}
\end{figure}

\section{Task 3}

The third task was to compress shuttle classes using side data that was also available during decompression.
There were 58000 class values to be compressed, each ranging from 1 to 7.
The side data consisted of 9 values for each shuttle class.

The side data didn't contain any duplicates with different classes.
That is, it was possible to create a perfect classifier that detected the class of each item based on just the side data without having to explicitly encode any data besides the classifier.

We created a simple decision tree learner to see how well we could classify the data with a simple tree.
Each node of the decision tree contained a test about whether some element of the side data was smaller than a given value, and each leave contained the class that were most common among the items in the noed.
We used the standard decision tree learning technique of choosing the test that maximizes the information gain of the test, which is equal to minimizing the entropy of the resulting two classes \cite{aima}.
The algorithm iterates over all possible ways to split the nodes and chooses the best way and recursively advances to split the resulting two sets.

The decision tree learning worked suprisingly well.
It turned out that the tree shown in Figure~\ref{fig:ex3small} with only 4 interior nodes was able to classify the data with precision 99.4379\%.
With 26 interior nodes the tree shown in Figure~\ref{fig:ex3big} was able to perfectly classify the input data.
The tree was so small that it didn't make sense to design any complex compression scheme for it.
Instead we just outputted the tree as C code to minimize the overhead of using the tree.
Thus our final code just read the side data from file and computed the classes using the code generated by the learning algorithm.

The C compiler produced quite a large code for the generated decision tree code, a few hundred bytes.
To reduce this size we changed from generating C code into storing the tree compactly in an array.
The array contains 4 bytes for each internal tree node: 1 byte for indicating each of the two children, 4 bits to indicate the variable to be tested and 12 bits for the value that the variable is compared against.
Thus the total data size of the tree of 26~nodes is just 104~bytes.
The size is so small that it didn't seem sensible to compress it more as any any decompression code would require more space than the saving archieved by compression.

\begin{figure}
    %\begin{center}
    \centerline{\includegraphics[width=1.3\textwidth]{ex3_big.pdf}}
    %\end{center}
    \caption{Exact decision tree}\label{fig:ex3big}
\end{figure}

\begin{figure}
    \begin{center}
    \includegraphics[scale=0.5]{ex3_small.pdf}
    \end{center}
    \caption{Approximate decision tree}\label{fig:ex3small}
\end{figure}

\section{Task 4}

In the fourth task we were asked to compress an image in wavelet transformed form. In the wavelet form noise spreads
uniformly over the whole matrix while the signal might have different distributions in different subbands.

The simplest model we tried was to fit normal distribution to each subband of the matrix and use arithmetic coding.
With this model the code length of the data was 240~KiB. The normal distribution fits the large subbands quite well, but not that well for the smaller ones, as can be seen in Figure~\ref{fig:histo}.

\begin{figure}
    \includegraphics[scale=0.5]{histo1}
    \includegraphics[scale=0.5]{histo15}
    \caption{Histograms of the frequencies of different values on two different subbands of the wavelet transform and normal distribution fitted to the data. The left figure shows one of the smallest subbands, where there is a lot of variation that doesn't fit well into normal distribution. The right figure contains the lower right biggest subband, which matches very well with the Gaussian distribution.}\label{fig:histo}
\end{figure}

The second model we tried was a mixture of normal distribution and generalized Gaussian distribution. This was more complex
as we need to compute the optimal $\beta$ and mixture ratio for each subband. We used nested ternary searches to find optimal parameters.
This took about 24 hours of computing on ukko cluster. Resulting code length was 239~KiB -- about one KiB less than with simple normal distributions.
The length of the decompression algorithm outweighted the gains in code length and the resulting executable size was slightly
greater than with the normal distribution model.

We also tried a simple adaptive model.
For each number processed we increased the probabilities of nearby values.
This produced code length of about 240~KiB.
By initializing the probabilities values from appropriate normal distribution we got the code length to about 239~KiB.
Again the increase in algorithm complexity outweighted the gains in code length.

\section{Problem 5}

The fifth task was to denoise a noisy image from its wavelet transform.

The basic approach for wavelet denoising is to do simple thresholding \cite{rvs02}. That is, assume that all values in wavelet data with absolute value below certain threshold are noise and set them to 0. We initially tried using a simple constant threshold for all the subbands.

We tried both hard and soft thresholding. Hard thresholding simply sets all values with absolute value less than threshold value $T$ to 0. Soft thresholding does that and also decreases the absolute value of other values by $T$. The results of those approaches are shown in Figure~\ref{fig:constt}. Both approaches either introduced a lot of artifacts to the image or made it very blurry, depending on the choise of $T$.

As constant thresholding didn't produce very good results we tried using the NormalShrink algorithm of \cite{ksc02}. The idea is to choose threshold for every subband $i$ using the formula $C/\sigma_i$, where $\sigma_i$ is the standard deviation of subband $i$ and $C$ is a constant that matches the overall variance of the image. After the threshold values have been computed, soft thresholding is used in each subband. The result is shown in Figure~\ref{fig:normalsht}.

\begin{figure}
    \centering
    \includegraphics[scale=0.40]{kd_hard}\includegraphics[scale=0.40]{kd_soft}
    \caption{Denoising using constant threshold for all bands. Left image shows the result of hard thresholding with threshold value 100, which creates quite a lot of artifacts. Right image uses soft thresholding with value 80, which decreases the amount of artifacts but makes the image quite blurry.}\label{fig:constt}
\end{figure}
\begin{figure}
    \centering
    \includegraphics[scale=0.4]{kd_normalshrink}
    \caption{Denoising using NormalShrink technique of \cite{ksc02}.}\label{fig:normalsht}
\end{figure}

\section{Optimizing the executable size}

As the goal was to minimize total package size, we decided to use C to have as small binary size as possible.
We also thought about the possibility of using a more high level language and including the source instead, but minimizing source size would have made coding quite stressing or required some special tools that we weren't familiar with.

On linux linking to the standard library increases binary size quite a lot,
so we avoided using it and implemented input and output in Assembly with system calls instead.
The arithmetic coding required division and the C~compiler generated calls to library functions from divisions, so we also replaced them by calls to a custom division function taken from the Linux kernel sources.

Even though we avoided using libraries as much as possible, we ended up linking against the standard math library.
The reason was that we needed the cumulative distribution function for the normal distribution for Gaussian models in tasks 2 and 4.
As there is no closed form solution for the CDF of normal distribution, it is not trivial to implement by hand, but it can easily be computed using the error function, which is included in the standard math library.



\bibliographystyle{abbrv}\bibliography{doc}

%\appendix

\end{document}
