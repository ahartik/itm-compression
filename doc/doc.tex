\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{url}

\newcommand{\code}{\texttt}

\title{Project in Information-Theoretic Modeling Final Report}
\author{Aleksi Hartikainen \and Mikko Sysikaski}

\begin{document}
\maketitle

%\abstract{}

%\newpage
%\tableofcontents
%\newpage

\section*{Problem 1}

The first task was to compress a sequence of bits using side data that was also available during decompression.

The file to be compressed had $100\,000$ binary digits, so trivially packing the bits yields a file of size $12\,500$ bytes.
For comparison, we also tried packing the data this way as the decompression code becomes much smaller and simpler.
This gave a total size of around 13.5~KiB uncompressed, and 12.5~KiB after compressing with gzip.

The task was to use arithmetic coding with probabilities from the given model.
Instead of using probabilities from model definition, we computed actual conditional frequencies from the data.
That is, for every tuple $(x_1,x_2,x_3,x_4)$ we counted how many times $x_0$ is 0 and 1, and this gave us the conditional probability of $x_0$ with respect to the other variables.
The calculated probabilities are shown in Figure~\ref{ex1_probs}.
We created a custom implementation of arithmetic coding that used the conditional probabilities of Figure~\ref{ex1_probs} to encode each bit.

Using arithmetic coding and the conditional probabilities, we got the data compressed in 8100~bytes.
Together with the decompression program, this gave a total file size of 9832~bytes.

To evaluate the correctness of our arithmetic coding implementation, we calculated also the theoretical bound for data size using entropy coding.
According to probabilities in Figure~\ref{ex1_probs}, the total entropy of the input file is 64790.4~bits, which is 8098.8~bytes, so the result generated by our implementation is within 1 byte of the theoretical optimum.


\begin{figure}
\label{ex1_probs}

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
$x_1$ & $x_2$ & $x_3$ & $x_4$ & $P(x_0 = 0)$ & $P(x_0 = 1) $ \\ \hline
   0 & 0 & 0 & 0 & 0.74244 & 0.25756 \\ \hline
   1 & 0 & 0 & 0 & 0.180631 & 0.819369 \\ \hline
   0 & 1 & 0 & 0 & 0.122035 & 0.877965 \\ \hline
   1 & 1 & 0 & 0 & 0.595336 & 0.404664 \\ \hline
   0 & 0 & 1 & 0 & 0.507643 & 0.492357 \\ \hline
   1 & 0 & 1 & 0 & 0.072675 & 0.927325 \\ \hline
   0 & 1 & 1 & 0 & 0.0461351 & 0.953865 \\ \hline
   1 & 1 & 1 & 0 & 0.359641 & 0.640359 \\ \hline
   0 & 0 & 0 & 1 & 0.888336 & 0.111664 \\ \hline
   1 & 0 & 0 & 1 & 0.37004 & 0.62996 \\ \hline
   0 & 1 & 0 & 1 & 0.277587 & 0.722413 \\ \hline
   1 & 1 & 0 & 1 & 0.78809 & 0.21191 \\ \hline
   0 & 0 & 1 & 1 & 0.733648 & 0.266352 \\ \hline
   1 & 0 & 1 & 1 & 0.177577 & 0.822423 \\ \hline
   0 & 1 & 1 & 1 & 0.119161 & 0.880839 \\ \hline
   1 & 1 & 1 & 1 & 0.582299 & 0.417701 \\ \hline
\end{tabular}
\end{center}
\caption{Conditional probabilities used in exercise 1}
\end{figure}

\section{Problem 2}

The second task statement was to compress four sequences of stocks prices over 100122 days.

The file had $10122 \times 4 = 40488$ prices with the precision of one cent (2 decimals).
All prices were under \$40 so they can be stored as 16-bit integers.
This gives total size of about 81~KiB.

Our first idea to compress the data was to use arithmetic coding and always write the difference to the last days price.
Probabilities of different 

\subsection {More complicated models}
We also tried fitting more complicated models to the data.
Simple polynomial fits gave significantly worse results than the first model.
Theoretical results using different polynomials can be seen in Figure \ref{ex2_polys}.

Next model was discrete cosine transform. This seemed to fit 

\begin{figure}
\label{ex2_polys}
\begin{center}
\begin{tabular}{|c|c|}
\hline
    Degree of polynomial & Code length in bits\\ \hline
    0      & 126663 \\  \hline
    1      & 123691 \\ \hline
    2      & 123563 \\ \hline
    3      & 120854 \\ \hline
    4      & 120852 \\ \hline
    5      & 120100 \\ \hline
    6      & 120131 \\ \hline
    30      & 114236 \\ \hline
\end{tabular}
\end{center}
\caption{Code length of first stock using polynomial model}
\end{figure}


\begin{enumerate}
\item Polynomial fit
Didn't work.
\item Discrete cosine transform
Didn't work either.

\end{enumerate}

\section{Optimizing the executable size}

As the goal was to minimize total package size, we decided to use C to have as small binary size as possible.
On linux linking to the standard library increases binary size quite a lot, so we avoided using it and implemented input and output in Assembly with system calls instead.




%\bibliographystyle{abbrv}\bibliography{ref}

%\appendix

\end{document}
